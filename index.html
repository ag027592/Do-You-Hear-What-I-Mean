<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Website for the paper 'Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-to-Speech Systems'.">
  <meta name="keywords" content="ITTS, TTS, Text-to-Speech, Expressive Speech, Speech Synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Do You Hear What I Mean?</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-to-Speech Systems</h1>
          <h3 class="title is-4 conference-name">ICASSP 2026</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=your-id">Yi-Cheng Lin</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=your-id">Huang-Cheng Chou</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=your-id">Tzu-Chieh Wei</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=your-id">Kuan-Yu Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=your-id">Hung-yi Lee</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Taiwan University,</span>
            <span class="author-block"><sup>2</sup>University of Southern California,</span>
            <span class="author-block"><sup>3</sup>University of Michigan</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/pdfs/your_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/your-username/your-repo-name"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://link-to-your-evoc-dataset.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>E-VOC Dataset</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="Teaser image showing analysis of TTS models."/>
      <h2 class="subtitle has-text-centered">
        We systematically evaluate the gap between natural language instructions and listener perception in modern Text-to-Speech systems across 5 expressive dimensions.
      </h2>
    </div>
  </div>
</section>
<!--/ Teaser -->

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Instruction-guided text-to-speech (ITTS) enables users to control speech generation through natural language prompts, offering a more intuitive interface than traditional TTS. However, it is unclear to what extent the instructions align with human perception. This work presents a perceptual analysis of ITTS controllability across 5 expressive dimensions (adverbs of degree, discrete emotion, graded emotion intensity, word-level emphasis, speaker age) and 3 acoustic dimensions (loudness, pitch, speaking rate). Our work uncovers significant gaps between intended instructions and perceived outcomes, as revealed by large-scale human evaluations. Our findings reveal the potential for significant enhancement in current ITTS models, particularly in how different emotion intensity in the instruction shapes controllability. To support reproducibility, we also describe an easy-to-follow data collection and analysis pipeline that can be applied to future ITTS systems. Our findings provide actionable insights into the limitations of current models and pathways toward more perceptually aligned speech synthesis.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Abstract -->

<!-- Audio Samples Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Audio Examples</h2>
    <div class="content has-text-justified">
      <p>
        Listen to examples showing how different models interpret the same instruction. How well do they align with your perception?
      </p>
    </div>

    <!-- Example 1: Happy -->
    <h3 class="title is-4">Instruction: "Speak in a <strong>very happy</strong> tone."</h3>
    <p><strong>Text:</strong> "You always make breakfast on Sundays."</p>
    <strong>gpt-4o-mini-tts:</strong> <audio controls src="static/audio/happy_gpt4o.wav" style="vertical-align: middle;"></audio><br/>
    <strong>Parler-TTS-large:</strong> <audio controls src="static/audio/happy_parler_large.wav" style="vertical-align: middle;"></audio><br/>
    <strong>PromptTTS++:</strong> <audio controls src="static/audio/happy_prompttts.wav" style="vertical-align: middle;"></audio><br/>
    <hr>

    <!-- Example 2: Sad -->
    <h3 class="title is-4">Instruction: "Speak in an <strong>extremely sad</strong> tone."</h3>
    <p><strong>Text:</strong> "Our meeting starts at nine sharp."</p>
    <strong>gpt-4o-mini-tts:</strong> <audio controls src="static/audio/sad_gpt4o.wav" style="vertical-align: middle;"></audio><br/>
    <strong>Parler-TTS-large:</strong> <audio controls src="static/audio/sad_parler_large.wav" style="vertical-align: middle;"></audio><br/>
    <strong>PromptTTS++:</strong> <audio controls src="static/audio/sad_prompttts.wav" style="vertical-align: middle;"></audio><br/>
    <hr>

    <!-- Example 3: Age -->
    <h3 class="title is-4">Instruction: "Speak like a <strong>child</strong>."</h3>
    <p><strong>Text:</strong> "Let's explore downtown tonight without plans."</p>
    <strong>gpt-4o-mini-tts:</strong> <audio controls src="static/audio/child_gpt4o.wav" style="vertical-align: middle;"></audio><br/>
    <strong>UniAudio:</strong> <audio controls src="static/audio/child_uniaudio.wav" style="vertical-align: middle;"></audio><br/>
    <strong>Parler-TTS-mini:</strong> <audio controls src="static/audio/child_parler_mini.wav" style="vertical-align: middle;"></audio><br/>
    <hr>
    
    <!-- Example 4: Emphasis -->
    <h3 class="title is-4">Instruction: "Articulate clearly, placing special stress on the term '<strong>borrow</strong>'."</h3>
    <p><strong>Text:</strong> "You might borrow my car later."</p>
    <strong>gpt-4o-mini-tts:</strong> <audio controls src="static/audio/emphasis_gpt4o.wav" style="vertical-align: middle;"></audio><br/>
    <strong>Parler-TTS-large:</strong> <audio controls src="static/audio/emphasis_parler_large.wav" style="vertical-align: middle;"></audio><br/>
    
  </div>
</section>
<!--/ Audio Samples Section -->


<!-- Evaluation Framework -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluation Framework</h2>
        <div class="content has-text-justified">
          <p>
            We carefully design a framework to quantify the alignment between instruction and perception. We define 5 key control dimensions, collect a new dataset (E-VOC) for analysis, and run large-scale human evaluations.
          </p>
          <h3 class="title is-4">Control Dimensions</h3>
          Our evaluation covers five expressive dimensions:
          <ul>
            <li><b>Adverbs of Degree (Adv. Deg.):</b> Can models interpret modifiers like "very" or "slightly" (e.g., "speak <i>very</i> happily")?</li>
            <li><b>Discrete Emotion (D-EMO):</b> Can models render distinct emotions like Happy, Sad, or Angry?</li>
            <li><b>Word-level Emphasis:</b> Can models selectively highlight a target word in a sentence?</li>
            <li><b>Speaker Age:</b> Can models generate speech that sounds like a child, teenager, adult, or elderly person?</li>
            <li><b>Emotion-Intensity Adjective (Emotion-I.A.):</b> Can models distinguish between graded emotions (e.g., "content" vs. "happy" vs. "ecstatic")?</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Evaluation Framework -->

<!-- Results -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>

    <!-- Figure 1 -->
    <h3 class="title is-4">Objective Acoustic Control</h3>
    <div class="content has-text-justified">
      <p>
        We first measure the objective acoustic properties (loudness, pitch, speaking rate) of the generated speech. Figure 1 shows that most models have good control over pitch and speaking rate when guided by adverbs of degree, but consistently struggle to modulate loudness based on instructions like "slightly quiet" or "extremely loud".
      </p>
      <div class="has-text-centered">
        <img src="static/images/figure1.png" alt="Figure 1 from the paper showing acoustic control results" class="center-image" width="80%"/>
        <p><b>Fig 1.</b> Loudness (LUFS), pitch (Hz), and speaking rate (words/s) across ITTS models for Task I. Adverbs of Degree.</p>
      </div>
    </div>
    <br>

    <!-- Figure 2 -->
    <h3 class="title is-4">Perceptual Emotion Intensity</h3>
    <div class="content has-text-justified">
      <p>
        Human evaluations reveal the gap between instructions and perception. Figure 2 shows the perceived emotion intensity for different models across four emotions. While some models follow the general trend (e.g., "ecstatic" is perceived as more intense than "happy"), there are significant discrepancies and overlaps, showing that fine-grained emotional control remains a major challenge.
      </p>
      <div class="has-text-centered">
        <img src="static/images/figure2.png" alt="Figure 2 from the paper showing perceptual emotion intensity" class="center-image" width="80%"/>
        <p><b>Fig 2.</b> Averaged perceptual emotion intensity of ITTS models across 4 emotions, analyzed by Adverbs of Degree and Emotion-Intensity Adjectives.</p>
      </div>
    </div>
  </div>
</section>
<!--/ Results -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{lin2026hearme,
  title     = {Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-to-Speech Systems},
  author    = {Lin, Yi-Cheng and Chou, Huang-Cheng and Wei, Tzu-Chieh and Chen, Kuan-Yu and Lee, Hung-yi},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2026}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
        <p>
        This page was built using the <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> template.
        </p>
    </div>
  </div>
</footer>

</body>
</html>